{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a13d40-610f-438f-924c-8cf473b22ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch \n",
    "from torch import nn, optim, tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import fastai\n",
    "from fastai.vision import *\n",
    "from fastai.vision import learner\n",
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b137c7e-7179-4397-a4c9-4914914271a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use gpu if available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f40c196-cdd6-4774-9828-ef418cbaefcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Data\n",
    "\n",
    "#Images\n",
    "imageNames = glob.glob('/home/lilcompaadres/Documents/Computer_Vision/bbc_train/pytorch_test/coco_dataset/imgs/*.jpg')\n",
    "imageNames.sort()\n",
    "imageNames.reverse()\n",
    "\n",
    "images = []\n",
    "for i in tqdm(imageNames[0:5000]):\n",
    "  img = cv2.imread(i)\n",
    "  img = cv2.resize(img, (438, 584))\n",
    "  images.append(img)\n",
    "\n",
    "#Masks and class label\n",
    "maskNames= glob.glob(\"/home/lilcompaadres/Documents/Computer_Vision/bbc_train/pytorch_test/coco_dataset/masks/*.png\")\n",
    "maskNames.sort()\n",
    "maskNames.reverse()\n",
    "masks = []\n",
    "class_labels = []\n",
    "\n",
    "for img in tqdm(maskNames[0:5000]):\n",
    "  mask = cv2.imread(img) \n",
    "  #mask = np.expand_dims(mask, axis=2)\n",
    "  mask = np.expand_dims(cv2.resize(mask, (438, 584))[:, :, 0], axis=2)\n",
    "  masks.append(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abe89dd-0237-413e-bd57-514faf1e17a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Data into Train / Test / Validation Datasets\n",
    "\n",
    "train_images = np.asarray(images[:4550], dtype=np.uint8)\n",
    "train_masks = np.asarray(masks[:4550], dtype=np.uint8)\n",
    "\n",
    "test_images = np.asarray(images[4651:4900], dtype=np.uint8)\n",
    "test_masks = np.asarray(masks[4651:4900], dtype=np.uint8)\n",
    "valid_images = np.asarray(images[4901:], dtype=np.uint8)\n",
    "valid_masks = np.asarray(masks[4901:], dtype=np.uint8)\n",
    "\n",
    "\n",
    "print(train_images.shape, train_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1139762-7f25-4649-8ed7-7346c0ad64e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Dataset\n",
    "class MergeData(Dataset):\n",
    "  def __init__(self, images, masks, c=1):\n",
    "    self.images = images\n",
    "    self.masks = masks\n",
    "    self.c = c\n",
    "    self.length = self.images.shape[0]\n",
    "    self.bbox = 1\n",
    "  \n",
    "  def __len__(self): \n",
    "    return self.length\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    img = self.images[idx, :, :, :]\n",
    "    mask = self.masks[idx, :, :, :]\n",
    "    \n",
    "    img = cv2.resize(img, (256, 256)) / 255.0\n",
    "    mask = cv2.resize(mask, (256, 256))\n",
    "    \n",
    "    mask = np.expand_dims(mask, 2)\n",
    "    \n",
    "\n",
    "    return tensor(img.transpose((2, 0, 1))).float(), tensor(mask.transpose((2, 0, 1)))\n",
    "\n",
    "  def show(self, idx):\n",
    "    #Bounding box rectangle\n",
    "    x, y = self.__getitem__(idx)\n",
    "    print(x.shape, y[0].shape)\n",
    "    print(x.dtype, y[0].dtype)\n",
    "\n",
    "    fig, ax = plt.subplots(1)\n",
    "    print(x.shape)\n",
    "    ax.imshow(x.numpy().transpose((1,2,0)))\n",
    "    print(y.numpy().transpose((1,2,0)).shape)\n",
    "    plt.imshow(y.numpy().transpose((1,2,0)), alpha=0.15)\n",
    "    plt.title(\"{}\".format(y[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1479491b-2a61-4769-9790-f3f05d7de3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Dataloader\n",
    "train = MergeData(train_images, train_masks)\n",
    "test = MergeData(test_images, test_masks)\n",
    "valid = MergeData(valid_images, valid_masks)\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test, batch_size=16, shuffle=True) \n",
    "valid_loader = DataLoader(valid, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca474f6e-0b18-4562-ac6e-36e68abdfd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Free up memory for unused variables\n",
    "%xdel train_images \n",
    "%xdel train_masks \n",
    "%xdel test_images\n",
    "%xdel test_masks\n",
    "%xdel valid_images\n",
    "%xdel valid_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2747ce5-74cb-4c8f-9e1c-4782c4e3cfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.show(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d41adee-f5d0-430e-91fd-35e1e533be24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_trans(ni, nf, ks = 4, stride = 2, padding = 1, ps=0.50):\n",
    "    return nn.Sequential(\n",
    "        nn.ConvTranspose2d(ni, nf, kernel_size=ks, bias=False, stride=stride, padding = padding), \n",
    "        nn.ReLU(inplace = True), \n",
    "        nn.BatchNorm2d(nf),\n",
    "        nn.Dropout(p=ps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2762497a-2d74-4185-abe0-22bd2a67c749",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "  def __init__(self, base_model, ps=0.35):\n",
    "    super(MyModel, self).__init__()\n",
    "    self.base_model = learner.create_body(base_model, pretrained=True)\n",
    "    self.seg_head = nn.Sequential(\n",
    "                                conv_trans(ni=512, nf=256, ks=4, stride=2, padding=1, ps=0.8), \n",
    "                                conv_trans(256, 128),\n",
    "                                conv_trans(128, 64),\n",
    "                                conv_trans(64, 32), \n",
    "                                nn.ConvTranspose2d(32, 1, kernel_size=4, bias=False, stride=2, padding = 1)                                  \n",
    "                                 ) \n",
    "    \n",
    "  def forward(self, x):\n",
    "    #Attach head to base model\n",
    "    x = self.base_model(x)\n",
    "    \n",
    "    #m3 = torch.nn.Softmax() \n",
    "    m3 = nn.Sigmoid()\n",
    "    seg_model = self.seg_head(m3(x))\n",
    "\n",
    "    return [seg_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29207959-a008-4855-827f-ad993cd65e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create custom loss function\n",
    "\n",
    "def MyLoss(yhat, seg_tgts):\n",
    "    #seg_loss = CrossEntropyLossFlat(axis=1)(yhat[0], seg_tgts.long()) #For multi-class problems\n",
    "    \n",
    "    seg_loss = torch.nn.BCELoss()(yhat, seg_tgts.float()) #BCEWithLogitLoss\n",
    "    return 1.0*seg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3d5294-9eb9-4929-b583-a322bb93c0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_accuracy(yhat, seg_tgts): #segmentation accuracy bbox_tgts, \n",
    "    #pred_mask[pred_mask>0.5] = 1\n",
    "    #pred_mask[pred_mask<0.5] = 0\n",
    "    #print(y.shape, seg_tgts.shape)\n",
    "    y_=seg_tgts.squeeze(dim=1)\n",
    "    yhat_=yhat.squeeze(dim=1)\n",
    "    \n",
    "    yhat_[yhat_>0.5] = 1\n",
    "    yhat_[yhat_<0.5] = 0\n",
    "    \n",
    "    return (y_==yhat_).sum().float()/seg_tgts.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fa107d-f24d-44f7-9b53-6fb42ae7483e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create model\n",
    "resnet_model = models.resnet18(pretrained=True)\n",
    "for param in resnet_model.parameters():\n",
    "    param.requires_grad = False #Freeze parameters so backwards doesn't affect weights\n",
    "\n",
    "model = MyModel(models.resnet18)\n",
    "model.to('cuda')\n",
    "print(\"Model Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf117819-f73f-41e5-9ac0-8e579cff8199",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = torch.load(\"model.pth\") #Load model\n",
    "NUM_EPOCHS = 80\n",
    "\n",
    "#Create optimizer\n",
    "loss = 0.0\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "#torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[8, 15, 20], gamma=0.1)\n",
    "best = 1000\n",
    "total = 0.0\n",
    "\n",
    "for epoch in range(0, NUM_EPOCHS):\n",
    "    print(f\"Epoch: {epoch} || Loss: {loss}\")\n",
    "    #model = torch.load(\"model.pth\")\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    training_accs = []\n",
    "    testing_accs = []\n",
    "    \n",
    "    for x, y in tqdm(train_loader):\n",
    "        x = x.to('cuda')\n",
    "        y = y.to('cuda')\n",
    "        \n",
    "        pred = model(x)\n",
    "        loss = MyLoss(torch.sigmoid(pred[0]), y) \n",
    "        training_accs.append(pixel_accuracy(pred[0], y))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "    #Compute model accuracy on training and testing data\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in tqdm(test_loader):\n",
    "            x = x.to('cuda')\n",
    "            y = y.to('cuda')\n",
    "            pred = model(x)\n",
    "            testing_accs.append(pixel_accuracy(pred[0], y))\n",
    "    \n",
    "    \n",
    "    print(f\"Training Accuracy: {sum(training_accs)/len(training_accs)}\")\n",
    "    print(f\"Testing Accuracy: {sum(testing_accs)/len(testing_accs)}\")\n",
    "      \n",
    "    \n",
    "    if loss < best:\n",
    "            best = loss\n",
    "            torch.save(model, \"model.pth\")\n",
    "            print(\"Saving model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5b3e0b-e33a-4906-97eb-f4293ae79e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_accs = []\n",
    "with torch.no_grad():\n",
    "        for x, y in tqdm(valid_loader):\n",
    "            x = x.to('cuda')\n",
    "            y = y.to('cuda')\n",
    "            pred = model(x)\n",
    "            valid_accs.append(pixel_accuracy(pred[0], y))\n",
    "print(f\"Testing Accuracy: {sum(valid_accs)/len(valid_accs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f1bbc6-4409-4391-8eed-34e3a651ba28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(model, image, gt_mask=None):\n",
    "    model.eval()\n",
    "    if gt_mask is None:\n",
    "        gt_mask = np.ones((1, 1))\n",
    "    with torch.no_grad():\n",
    "        print(gt_mask.sum())\n",
    "        image = image.astype(\"float32\") / 255.0 if gt_mask.sum() == 1 else image.astype(\"float32\")\n",
    "        image = cv2.resize(image, (256, 256))\n",
    "\n",
    "        orig_image = image.copy()\n",
    "\n",
    "        #gt_mask = cv2.resize(mask, (256, 256)) if gt_mask.sum() > 0 else None\n",
    "\n",
    "        image = np.transpose(image, (2, 0, 1)) #Move channel axis to front\n",
    "        image = np.expand_dims(image, 0)\n",
    "\n",
    "        #Make prediction from image input\n",
    "        image = torch.from_numpy(image).to(\"cuda\")\n",
    "        pred_mask = model(image)[0].squeeze()\n",
    "        \n",
    "        pred_mask = torch.sigmoid(pred_mask)\n",
    "        pred_mask = pred_mask.cpu().numpy()\n",
    "        \"\"\"arr = np.ones((256, 256), np.uint8)\n",
    "        arr[pred_mask>0.15] = 0\n",
    "        arr[pred_mask<0.15] = 1\n",
    "        print(np.unique(arr))\n",
    "        orig_image[:,:,0] *= arr\n",
    "        orig_image[:,:,1] *= arr\n",
    "        orig_image[:,:,2] *= arr\"\"\"\n",
    "        f, axarr = plt.subplots(1,3, figsize=(15,15))\n",
    "        \n",
    "        axarr[0].title.set_text('Original Image')\n",
    "        axarr[1].title.set_text('GT Segmentation Mask')\n",
    "        axarr[2].title.set_text('Predicted Segmentation Mask')\n",
    "        axarr[0].imshow(cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB))\n",
    "        axarr[1].imshow(gt_mask) #if gt_mask is None else None\n",
    "        axarr[2].imshow(pred_mask)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856620a6-488b-45ff-b54f-c66e6d9270f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"person.png\")\n",
    "\n",
    "predict_image(model, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f10417-2369-4b2a-929a-9b7c6b94dd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 10\n",
    "\n",
    "batch = test.__getitem__(num) \n",
    "\n",
    "img = np.transpose(batch[0], (1, 2, 0)).cpu().numpy()\n",
    "\n",
    "gt_mask = np.moveaxis(batch[1].numpy(), 0, -1)\n",
    "\n",
    "predict_image(model, img, gt_mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
